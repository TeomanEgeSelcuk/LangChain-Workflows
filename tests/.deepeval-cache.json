{"test_cases_lookup_map": {"{\"actual_output\": \"\\n            The coverage score quantifies how well a summary captures and\\n            accurately represents key information from the original text,\\n            with a higher score indicating greater comprehensiveness.\\n            \", \"context\": [\"Coverage score Checks for accuracy of representation\", \"Coverage score ensures comprehensiveness of summaries\", \"Coverage score evaluates summaries for key information\", \"Coverage score focuses on faithfulness to original text\"], \"expected_output\": \"The coverage score is calculated as the percentage of assessment questions for which both the summary and the original document provide a 'yes' answer. This method ensures that the summary includes key information from the original text and accurately represents it. A higher coverage score indicates a more comprehensive and faithful summary, effectively encapsulating crucial points and details from the original content.\", \"hyperparameters\": null, \"input\": \"The 'coverage score' is calculated as the percentage of assessment questions for which both the summary and the original document provide a 'yes' answer. This method ensures that the summary not only includes key information from the original text but also accurately represents it. A higher coverage score indicates a more comprehensive and faithful summary, signifying that the summary effectively encapsulates the crucial points and details from the original content.\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_metadata": {"metric": "Correctness (GEval)", "threshold": 0.45, "success": false, "score": 0.40294852361962674, "reason": "The actual output captures the general idea but omits specifics on the calculation method and details on the 'yes' answer comparison from the expected output.", "strictMode": false, "evaluationModel": "gpt-4o", "evaluationCost": 0}, "metric_configuration": {"threshold": 0.45, "evaluation_model": "gpt-4o", "strict_mode": false, "criteria": "Determine whether the actual output is factually correct based on the expected output.", "include_reason": false, "evaluation_steps": ["Check whether the facts in 'actual output' contradict any facts in 'expected output'", "You should also heavily penalize omission of detail", "Vague language, or contradicting OPINIONS, are OK"], "evaluation_params": ["input", "actual_output", "expected_output", "context"]}}, {"metric_metadata": {"metric": "Summarization", "threshold": 0.5, "success": false, "score": 0.3333333333333333, "reason": "The score is 0.33 because the summary fails to address specific questions that the original text can answer, such as whether the coverage score is based on a percentage of 'yes' answers and if the score ensures the summary's accuracy with the source.", "strictMode": false, "evaluationModel": "gpt-4o", "evaluationCost": 0}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "gpt-4o", "strict_mode": false, "include_reason": true, "n": 5, "assessment_questions": ["Is the coverage score based on a percentage of 'yes' answers?", "Does the score ensure the summary's accuracy with the source?", "Does a higher score mean a more comprehensive summary?"]}}]}, "{\"actual_output\": \"\\n            The alignment score checks if the summary is consistent with the original text.\\n            \", \"context\": [\"Alignment score ensures no hallucinated or contradictory information is introduced.\", \"This alignment score  evaluates the consistency of the summary with the original document.\"], \"expected_output\": \"The alignment score determines whether the summary contains hallucinated or contradictory information to the original text. This method ensures that the summary is consistent with the original document and does not introduce new information that is not present in the original text.\", \"hyperparameters\": null, \"input\": \"The 'alignment score' determines whether the summary contains hallucinated or contradictory information to the original text. This method ensures that the summary is consistent with the original document and does not introduce new information that is not present in the original text.\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_metadata": {"metric": "Correctness (GEval)", "threshold": 0.45, "success": false, "score": 0.2502323268545448, "reason": "The actual output is missing key details about hallucinated or contradictory information, which are critical in the expected output and context. It only vaguely mentions consistency.", "strictMode": false, "evaluationModel": "gpt-4o", "evaluationCost": 0}, "metric_configuration": {"threshold": 0.45, "evaluation_model": "gpt-4o", "strict_mode": false, "criteria": "Determine whether the actual output is factually correct based on the expected output.", "include_reason": false, "evaluation_steps": ["Check whether the facts in 'actual output' contradict any facts in 'expected output'", "You should also heavily penalize omission of detail", "Vague language, or contradicting OPINIONS, are OK"], "evaluation_params": ["input", "actual_output", "expected_output", "context"]}}, {"metric_metadata": {"metric": "Summarization", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The score is 1.00 because the summary perfectly aligns with the original text. There are no contradictions or extraneous details, indicating a high-quality summarization. Excellent job!", "strictMode": false, "evaluationModel": "gpt-4o", "evaluationCost": 0}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "gpt-4o", "strict_mode": false, "include_reason": true, "n": 5, "assessment_questions": ["Is the coverage score based on a percentage of 'yes' answers?", "Does the score ensure the summary's accuracy with the source?", "Does a higher score mean a more comprehensive summary?"]}}]}}}