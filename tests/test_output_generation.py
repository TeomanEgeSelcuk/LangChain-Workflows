## LLM UNIT TESTS
# Use command: deepeval test run tests/test_output_generation.py to run the tests

"""
This script evaluates the performance of various LLM models using the G-Eval and Summarization metrics from the DeepEval framework.
--------------
G-Eval:
G-Eval is a highly adaptable evaluation framework that uses chain-of-thought (CoT) reasoning to assess the accuracy and quality of 
LLM outputs based on custom criteria. It can be applied to almost any use case, providing human-like accuracy. 
To utilize G-Eval, you need to create an LLMTestCase with the following arguments:
- input: The original text or prompt given to the model.
- actual_output: The response generated by the model.
- expected_output: The ideal or correct response for comparison.
- context: Any relevant information that may influence the evaluation.

You define the evaluation criteria in plain language or through a series of detailed steps. 
G-Eval can either generate evaluation steps based on the provided criteria or use predefined steps to guide the evaluation process.
--------------
Summarization:
The Summarization metric evaluates whether an LLM-generated summary is both factually accurate and comprehensive. 
This metric ensures that the summary includes all necessary details from the original text without introducing errors. 
For a Summarization LLMTestCase, you need to provide:
- input: The original text to be summarized.
- actual_output: The summary generated by the model.

Optional parameters for Summarization include threshold, assessment_questions, model, include_reason, strict_mode, and async_mode.
 The final score is the minimum of the alignment_score (checking for hallucinations and contradictions) and the
   coverage_score (ensuring key details from the original text are included).
"""

from deepeval import evaluate
from deepeval.metrics import SummarizationMetric, GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from langchain_env.output_generation import generate_output  # Importing the function

# Define the test cases for summarization
functions_test_cases = [
    ("Summarize", """
    The 'coverage score' is calculated as the percentage of assessment questions
    for which both the summary and the original document provide a 'yes' answer.
    This method ensures that the summary not only includes key information from the original
    text but also accurately represents it. A higher coverage score indicates a
    more comprehensive and faithful summary, signifying that the summary effectively
    encapsulates the crucial points and details from the original content.
    """),
    ("Summarize", """
    The 'alignment score' determines whether the summary contains hallucinated or
    contradictory information to the original text. This method ensures that the
    summary is consistent with the original document and does not introduce new
    information that is not present in the original text.
    """)
]

def generate_test_cases(response):
    # Extract the 'text' values
    texts = [item[2]['text'] for item in response]
    # Define test cases for different models and engines
    test_cases = [
        LLMTestCase(
            input=texts[0],
            actual_output="""
            The coverage score quantifies how well a summary captures and
            accurately represents key information from the original text,
            with a higher score indicating greater comprehensiveness.
            """,
            expected_output="The coverage score is calculated as the percentage of assessment questions for which both the summary and the original document provide a 'yes' answer. This method ensures that the summary includes key information from the original text and accurately represents it. A higher coverage score indicates a more comprehensive and faithful summary, effectively encapsulating crucial points and details from the original content.",
            context= ["Coverage score evaluates summaries for key information",
                        "Coverage score Checks for accuracy of representation",
                        "Coverage score ensures comprehensiveness of summaries",
                        "Coverage score focuses on faithfulness to original text"] 
    ),
        LLMTestCase(
            input=texts[1],
            actual_output="""
            The alignment score checks if the summary is consistent with the original text.
            """,
            expected_output="The alignment score determines whether the summary contains hallucinated or contradictory information to the original text. This method ensures that the summary is consistent with the original document and does not introduce new information that is not present in the original text.",
            context=["This alignment score  evaluates the consistency of the summary with the original document.", 
                     "Alignment score ensures no hallucinated or contradictory information is introduced."]
        ),
    ]
    return test_cases

# Define the G-Eval metric
correctness_metric = GEval(
    name="Correctness",
    criteria="Determine whether the actual output is factually correct based on the expected output.",
    evaluation_steps=[
        "Check whether the facts in 'actual output' contradict any facts in 'expected output'",
        "You should also heavily penalize omission of detail",
        "Vague language, or contradicting OPINIONS, are OK"
    ],
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT, LLMTestCaseParams.CONTEXT],
    threshold=0.45, model="gpt-3.5-turbo-0125"
)

# Define the summarization metric
summarization_metric = SummarizationMetric(threshold=0.5, model="gpt-3.5-turbo-0125", assessment_questions=[
    "Is the coverage score based on a percentage of 'yes' answers?",
    "Does the score ensure the summary's accuracy with the source?",
    "Does a higher score mean a more comprehensive summary?"
])


# NOTE: Rate limits may exceed and the outputs might cause an error due to the API restrictions.

# Evaluate test cases for OpenRouter default model (nousresearch/nous-capybara-34b)
print("Evaluating OpenRouter default model (nousresearch/nous-capybara-34b):")
results_openrouter_default = generate_output(tasks=functions_test_cases, engine="openrouter")
test_case_openrouter_default = generate_test_cases(results_openrouter_default)
score_output_openrouter_default = evaluate(test_case_openrouter_default, [correctness_metric, summarization_metric])

# Evaluate test cases for OpenRouter phi-3-mini-128k-instruct model:
print("Evaluating OpenRouter phi-3-mini-128k-instruct model:")
results_phi3mini = generate_output(tasks=functions_test_cases, engine="openrouter", model_name='microsoft/phi-3-mini-128k-instruct')
test_case_phi3mini = generate_test_cases(results_phi3mini)
score_output_phi3mini = evaluate(test_case_phi3mini, [correctness_metric, summarization_metric])

# Evaluate test cases for Groq gemma-7b-it model
print("Evaluating Groq gemma-7b-it model:")
results_gemma7b = generate_output(tasks=functions_test_cases, engine="groq", model_name='gemma-7b-it')
test_case_gemma7b = generate_test_cases(results_gemma7b)
score_output_gemma7b = evaluate(test_case_gemma7b, [correctness_metric, summarization_metric])

# Evaluate test cases for Groq default model (llama3-8b-8192)
print("Evaluating Groq default model (llama3-8b-8192):")
results_groq_default = generate_output(tasks=functions_test_cases, engine="groq")
test_case_groq_default = generate_test_cases(results_groq_default)
score_output_groq_default = evaluate(test_case_groq_default, [correctness_metric, summarization_metric])

# Evaluate test cases for OpenAI default model (gpt-3.5-turbo-0125)
print("Evaluating OpenAI default model (gpt-3.5-turbo-0125):")
results_openai_default = generate_output(tasks=functions_test_cases, engine="openai")
test_case_openai_default = generate_test_cases(results_openai_default)
score_output_openai_default = evaluate(test_case_openai_default, [correctness_metric, summarization_metric])
